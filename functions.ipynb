{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c5bf303",
   "metadata": {},
   "source": [
    "# Functions\n",
    "\n",
    "## This script contains the following\n",
    "\n",
    "* Imports libraries and initializes figure parameters\n",
    "* Imports functions\n",
    "\n",
    "## The following functions are included:\n",
    "\n",
    "* UFL_to_csc\n",
    "* UFL_to_csc_nobcs\n",
    "* POD\n",
    "* POD2\n",
    "* POD_all\n",
    "* POD_M1\n",
    "* plot_first_n_svalues\n",
    "* map_nodal_to_vertex\n",
    "* block_vector_to_nodal\n",
    "* U_to_figure\n",
    "* nodal_to_figure\n",
    "* split_data\n",
    "* plot_last_epochs\n",
    "* relative_error\n",
    "* my_2norm\n",
    "* get_inputs\n",
    "* get_outputs\n",
    "* input_to_output_model_nonorm\n",
    "* find_initial_condition\n",
    "* newdescent\n",
    "* newton_nonorm\n",
    "* Neural_Network\n",
    "* Neural_Network_small\n",
    "* train_nn\n",
    "* BFGS_Fu\n",
    "* BFGS_g\n",
    "* BFGS_grad_g\n",
    "* BFGS_line_search\n",
    "* BFGS\n",
    "* Fu\n",
    "* g\n",
    "* grad_g\n",
    "* line_search\n",
    "* standardize\n",
    "* normalize\n",
    "* Monte_Carlo\n",
    "* f_normal\n",
    "* uni_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "caba634c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------- Import libraries\n",
    "\n",
    "from dolfin import *\n",
    "import numpy as np\n",
    "import scipy.sparse as sps\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from re import X\n",
    "\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "import logging\n",
    "\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "from mpl_toolkits.axes_grid1 import AxesGrid\n",
    "\n",
    "logging.getLogger('FFC').setLevel(logging.WARNING)\n",
    "logging.getLogger('UFL').setLevel(logging.WARNING)\n",
    "set_log_level(30) # Level 30: WARNING\n",
    "\n",
    "params = {\n",
    "        'axes.labelsize': 21,\n",
    "        'font.size': 21,\n",
    "        'legend.fontsize': 15,\n",
    "        'xtick.labelsize': 15,\n",
    "        'ytick.labelsize': 15,\n",
    "        'text.usetex': False,\n",
    "        'axes.linewidth': 2,\n",
    "        'xtick.major.width': 2,\n",
    "        'ytick.major.width': 2,\n",
    "        'xtick.major.size': 2,\n",
    "        'ytick.major.size': 2,\n",
    "    }\n",
    "\n",
    "plt.rcParams.update(params)\n",
    "\n",
    "# --------------------- End import libraries\n",
    "\n",
    "\n",
    "def UFL_to_csc(a,f,bcs):\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    Assembles a weak form in UFL-form to a scipy sparse matrix and numpy array\n",
    "\n",
    "    Inputs:\n",
    "\n",
    "        a (UFL): UFL-form generated by dolfin\n",
    "        f (UFL): UFL-form generated by dolfin\n",
    "\n",
    "    Outputs:\n",
    "\n",
    "        A (sps.csc_matrix): sparse matrix to be read by scipy\n",
    "        F_mat (numpy.ndarray): numpy array \n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    A_PET = PETScMatrix()\n",
    "    F_PET = PETScVector()\n",
    "    \n",
    "    assemble_system(a, f, bcs, A_tensor=A_PET, b_tensor=F_PET) \n",
    "    \n",
    "    # convert to petsc4py object\n",
    "\n",
    "    A_mat = A_PET.mat()\n",
    "    \n",
    "    F_mat = F_PET.get_local() \n",
    "\n",
    "\n",
    "    # convert to csr_matrix\n",
    "\n",
    "    (A_indptr, A_ind, A_data) = A_mat.getValuesCSR()\n",
    "\n",
    "    A = sps.csc_matrix((A_data, A_ind, A_indptr), shape=A_mat.getSize())\n",
    "\n",
    "    A.eliminate_zeros()\n",
    "\n",
    " \n",
    "\n",
    "    # return coo_matrix\n",
    "\n",
    "    return A, F_mat\n",
    "\n",
    "def UFL_to_csc_nobcs(a,f):\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    Assembles a weak form in UFL-form to a scipy sparse matrix and numpy array\n",
    "\n",
    "    Inputs:\n",
    "\n",
    "        a (UFL): UFL-form generated by dolfin\n",
    "        f (UFL): UFL-form generated by dolfin\n",
    "\n",
    "    Outputs:\n",
    "\n",
    "        A (sps.csc_matrix): sparse matrix to be read by scipy\n",
    "        F_mat (numpy.ndarray): numpy array \n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    A_PET = PETScMatrix()\n",
    "    F_PET = PETScVector()\n",
    "    \n",
    "    assemble_system(a, f, A_tensor=A_PET, b_tensor=F_PET) \n",
    "    \n",
    "    # convert to petsc4py object\n",
    "\n",
    "    A_mat = A_PET.mat()\n",
    "    \n",
    "    F_mat = F_PET.get_local() \n",
    "\n",
    "\n",
    "    # convert to csr_matrix\n",
    "\n",
    "    (A_indptr, A_ind, A_data) = A_mat.getValuesCSR()\n",
    "\n",
    "    A = sps.csc_matrix((A_data, A_ind, A_indptr), shape=A_mat.getSize())\n",
    "\n",
    "    A.eliminate_zeros()\n",
    "\n",
    "    # return coo_matrix\n",
    "\n",
    "    return A, F_mat\n",
    "\n",
    "def POD(S,threshold,plot):\n",
    "    \"\"\" \n",
    "    Performs a Proper Orthogonal Decomposition (POD) on a snapshot matrix\n",
    "\n",
    "    Inputs: \n",
    "\n",
    "    S (numpy array) - snapshot matrix\n",
    "    threshold (float) - cutoff value for the singular values\n",
    "    plot (boolean) - plot for singular values\n",
    "\n",
    "    Outputs : \n",
    "\n",
    "    V_r (numpy array) - transformation matrix\n",
    "    s = (numpy array) - singular values\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # SVD decomposition\n",
    "    u, s, vh = np.linalg.svd(S, full_matrices=True)\n",
    "    \n",
    "    # Plot Singular Values\n",
    "    if plot:\n",
    "        xaxis = np.arange(1,len(s)+1)\n",
    "\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(2, 1, 1)\n",
    "        ax.set_yscale('log')\n",
    "\n",
    "        ax.plot(xaxis, s)\n",
    "        ax.set_ylabel('Singular value')\n",
    "        ax.set_xlabel('Index')\n",
    "\n",
    "    # Define threshold so that we stop picking singular values below threshold\n",
    "\n",
    "    for cutoff_index in range(0,len(s)):\n",
    "        if s[cutoff_index] < threshold:\n",
    "            break\n",
    "\n",
    "    V_r = u[:,:cutoff_index+1]\n",
    "\n",
    "    return V_r, s\n",
    "\n",
    "def POD2(S,threshold : int ,plot):\n",
    "    \"\"\" \n",
    "    Performs a Proper Orthogonal Decomposition (POD) on a snapshot matrix\n",
    "\n",
    "    Inputs: \n",
    "\n",
    "    S (numpy array) - snapshot matrix\n",
    "    threshold (integer) - cutoff value for the singular values\n",
    "    plot (boolean) - plot for singular values\n",
    "\n",
    "    Outputs : \n",
    "\n",
    "    V_r (numpy array) - transformation matrix\n",
    "    s = (numpy array) - singular values\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # SVD decomposition\n",
    "    u, s, vh = np.linalg.svd(S, full_matrices=True)\n",
    "    \n",
    "    # Plot Singular Values\n",
    "    if plot:\n",
    "        xaxis = np.arange(1,len(s)+1)\n",
    "\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(2, 1, 1)\n",
    "        ax.set_yscale('log')\n",
    "\n",
    "        ax.plot(xaxis, s)\n",
    "        ax.set_ylabel('Singular value')\n",
    "        ax.set_xlabel('Index')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    # Define threshold so that we stop picking singular values below threshold\n",
    "\n",
    "    V_r = u[:,:threshold]\n",
    "\n",
    "    return V_r, s\n",
    "\n",
    "def POD_all(S1, SI, S2, cutoff1 : int, cutoffI : int, cutoff2 : int):\n",
    "    \"\"\" \n",
    "    Performs a Proper Orthogonal Decomposition (POD) on a snapshot matrix\n",
    "\n",
    "    Inputs: \n",
    "\n",
    "    S1, SI, S2 (numpy array) - snapshot matrices\n",
    "    cutoff1, cutoffI, cutoff2 (integer) - cutoff values for the singular values\n",
    "\n",
    "    Outputs : \n",
    "    \n",
    "    V_1, V_I, V_2 (numpy array) - transformation matrices\n",
    "    ndofs_u1, ndofs_uI, ndofs_u2 (integer) - DoFs in reduced spaces\n",
    "    u1, uI, u2 (numpy array) - reduced-order snapshot matrices\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    S1_train = S1[:,0:nsamples-ntest]\n",
    "    SI_train = SI[:,0:nsamples-ntest]\n",
    "    S2_train = S2[:,0:nsamples-ntest]\n",
    "\n",
    "    start = time.time()\n",
    "    V_1, svs_1 = POD2(S1_train,cutoff1,0)\n",
    "    V_I, svs_I = POD2(SI_train,cutoffI,0)\n",
    "    end = time.time()\n",
    "    \n",
    "    POD1I_time = end-start\n",
    "    \n",
    "    start = time.time()\n",
    "    V_2, svs_2 = POD2(S2_train,cutoff2,0)\n",
    "    end = time.time()\n",
    "    \n",
    "    PODall_time = end-start+POD1I_time\n",
    "    \n",
    "    ndofs_u1 = np.shape(V_1)[1]\n",
    "    ndofs_uI = np.shape(V_I)[1]\n",
    "    ndofs_u2 = np.shape(V_2)[1]\n",
    "\n",
    "    u1 = np.matmul(S1.T,V_1)\n",
    "    uI = np.matmul(SI.T,V_I)\n",
    "    u2 = np.matmul(S2.T,V_2)\n",
    "    \n",
    "    energy1 = sum(svs_1[0:cutoff1])/sum(svs_1)\n",
    "    energy2 = sum(svs_2[0:cutoff2])/sum(svs_2)\n",
    "    energyI = sum(svs_I[0:cutoffI])/sum(svs_I)\n",
    "    \n",
    "    print(\"Snapshot energy domain 1:\",energy1)\n",
    "    print(\"Snapshot energy domain 2:\",energy2)\n",
    "    print(\"Snapshot energy interface:\",energyI)\n",
    "    \n",
    "    ## FIGURE ##\n",
    "    \n",
    "    xaxis1 = np.arange(1,len(svs_1)+1)\n",
    "    xaxis2 = np.arange(1,len(svs_2)+1)\n",
    "    xaxisI = np.arange(1,len(svs_I)+1)\n",
    "\n",
    "\n",
    "    fig = plt.figure(figsize=(6.4, 4.8))\n",
    "    plt.yscale('log')\n",
    "    plt.xscale('log')\n",
    "\n",
    "    plt.plot(xaxis1, svs_1, label='$\\sigma_{\\Omega_1, i}$', marker='o', linewidth = 2)\n",
    "    plt.plot(xaxis2, svs_2, label='$\\sigma_{\\Omega_2, j}$', marker='o', linewidth = 2)\n",
    "    plt.plot(xaxisI, svs_I, label='$\\sigma_{\\Gamma, k}$', marker='o', linewidth = 2)\n",
    "\n",
    "    plt.legend()        \n",
    "    plt.grid(True, which='major', linestyle='-')\n",
    "    plt.grid(True, which='minor', linestyle='--')\n",
    "    \n",
    "    plt.savefig('svd', bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return V_1, V_I, V_2, ndofs_u1, ndofs_uI, ndofs_u2, u1, uI, u2\n",
    "\n",
    "def POD_M1(S1, SI, nsamples : int, ntest : int, cutoff1 : int, cutoffI : int):\n",
    "    \"\"\"\n",
    "    Performs POD analysis on domain 1 and the interface\n",
    "    \n",
    "    Inputs: \n",
    "\n",
    "    S1, SI (numpy array) - snapshot matrices\n",
    "    nsamples (integer) - number of samples\n",
    "    ntest (integer) - number of test samples\n",
    "    cutoff1, cutoffI (integer) - cutoff values for the singular values\n",
    "\n",
    "    Outputs : \n",
    "    \n",
    "    V_1, V_I (numpy array) - transformation matrices\n",
    "    ndofs_u1, ndofs_uI (integer) - DoFs in reduced spaces\n",
    "    PODtime (float) - computation time\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    S1_train = S1[:,0:nsamples-ntest]\n",
    "    SI_train = SI[:,0:nsamples-ntest]\n",
    "\n",
    "    V_1, svs_1 = POD2(S1_train,cutoff1,0)\n",
    "    V_I, svs_I = POD2(SI_train,cutoffI,0)\n",
    "\n",
    "    ndofs_u1 = np.shape(V_1)[1]\n",
    "    ndofs_uI = np.shape(V_I)[1]\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    PODtime = start - end\n",
    "    \n",
    "    print(\"We go from\",ndofs_U1,\"to\",ndofs_u1,\"modes on domain 1\")\n",
    "    print(\"We go from\",ndofs_UI,\"to\",ndofs_uI,\"modes on the interface\")\n",
    "    \n",
    "    return V_1, V_I, ndofs_u1, ndofs_uI, PODtime\n",
    "\n",
    "def plot_first_n_svalues(n,s):\n",
    "    \"\"\"\n",
    "    Plots the first n singular values\n",
    "    \n",
    "    Inputs: \n",
    "    \n",
    "    n (positive integer) - first n singular values\n",
    "    s (numpy array) - singular values\n",
    "    \n",
    "    Outputs: \n",
    "    \n",
    "    Figure of the first n singular values\n",
    "    \n",
    "    \"\"\"\n",
    "    xaxis = np.arange(1,n+1)\n",
    "    fig = plt.figure()\n",
    "    \n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    ax.plot(xaxis,s[0:n])\n",
    "    ax.set_yscale('log')\n",
    "    ax.set_ylabel('Singular value')\n",
    "    ax.set_xlabel('Index')\n",
    "\n",
    "def map_nodal_to_vertex(u,mesh):\n",
    "    \"\"\"\n",
    "    \n",
    "    Maps the nodal values to the vertex values\n",
    "    \n",
    "    Inputs: \n",
    "    \n",
    "    u (numpy array) - nodal values\n",
    "    \n",
    "    Outputs: \n",
    "    \n",
    "    u (numpy array) - vertex values\n",
    "    \n",
    "    \"\"\"\n",
    "    #V1 = FunctionSpace(mesh,\"Lagrange\", 1)\n",
    "    mapping = vertex_to_dof_map(V)\n",
    "    return u[mapping]\n",
    "\n",
    "def block_vector_to_nodal(u_final,rows_domain1,rows_domain2,rows_interface,E_1,E_2,E_interface):\n",
    "    \n",
    "    \"\"\"\n",
    "    Rearranges the solution vector u = (u_1, u_2, u_\\Gamma) to the nodal values\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    u_final (numpy array) - solution vector u\n",
    "    \n",
    "    rows_domain1 (numpy array) - contains the indices of the dofs on domain 1\n",
    "    rows_domain2 (numpy array) - contains the indices of the dofs on domain 2\n",
    "    rows_interface (numpy array) - contains the indices of the dofs on the interface\n",
    "    \n",
    "    E_1 (sps.csc_matrix ) - sparse matrix that rearranges the dofs on domain 1\n",
    "    E_2 (sps.csc_matrix) - sparse matrix that rearranges the dofs on domain 2\n",
    "    E_interface (sps.csc_matrix) - sparse matrix that rearranges the dofs on the interface\n",
    "    \n",
    "    Outputs\n",
    "    \n",
    "    v_final (numpy array) - solution vector as nodal values\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    u_part1 = u_final[:len(rows_domain1)]\n",
    "    u_part2 = u_final[len(rows_domain1):len(rows_domain1)+len(rows_domain2)]\n",
    "    u_partI = u_final[-len(rows_interface):]\n",
    "\n",
    "    # Map back to nodal values\n",
    "    v_final = E_1*u_part1 + E_2*u_part2 + E_interface*u_partI\n",
    "    return v_final\n",
    "\n",
    "def U_to_figure(U,rows_domain1,rows_domain2,rows_interface,E_1,E_2,E_interface,mesh,gridsize):\n",
    "    \n",
    "    \"\"\"\n",
    "    Creates a figure from the solution vector u = (u_1, u_2, u_\\Gamma)\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    U (numpy array) - solution vector u\n",
    "    \n",
    "    rows_domain1 (numpy array) - contains the indices of the dofs on domain 1\n",
    "    rows_domain2 (numpy array) - contains the indices of the dofs on domain 2\n",
    "    rows_interface (numpy array) - contains the indices of the dofs on the interface\n",
    "    \n",
    "    E_1 (sps.csc_matrix ) - sparse matrix that rearranges the dofs on domain 1\n",
    "    E_2 (sps.csc_matrix) - sparse matrix that rearranges the dofs on domain 2\n",
    "    E_interface (sps.csc_matrix) - sparse matrix that rearranges the dofs on the interface\n",
    "    \n",
    "    mesh (dolfin.cpp.generation.UnitSquareMesh) - contains the mesh \n",
    "    gridsize (positive integer) - number of dofs per row on the mesh\n",
    "    \n",
    "    Outputs:\n",
    "    \n",
    "    v_final (numpy array) - solution vector as nodal values\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    U_nodal = block_vector_to_nodal(U,rows_domain1,rows_domain2,rows_interface,E_1,E_2,E_interface)\n",
    "    U_vertex = map_nodal_to_vertex(U_nodal,mesh)\n",
    "    u_reshape = np.reshape(U_vertex,(gridsize+1, gridsize+1))\n",
    "    u_reshape = np.flipud(u_reshape)\n",
    "\n",
    "    # Plot FOM\n",
    "    plt.figure(figsize=(6.4, 4.8))\n",
    "    plt.imshow(u_reshape, interpolation = \"nearest\",extent=[0,1,0,1], cmap='jet')\n",
    "    plt.colorbar()\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.show()\n",
    "    \n",
    "def block_to_fig(U,rows_domain1,rows_domain2,rows_interface,E_1,E_2,E_interface,mesh,gridsize):\n",
    "    \n",
    "    \"\"\"\n",
    "    Transforms the solution vector U from the block structure to an array to plot\n",
    "    \n",
    "    Inputs: \n",
    "    \n",
    "    U (numpy array) - solution vector u\n",
    "    \n",
    "    rows_domain1 (numpy array) - contains the indices of the dofs on domain 1\n",
    "    rows_domain2 (numpy array) - contains the indices of the dofs on domain 2\n",
    "    rows_interface (numpy array) - contains the indices of the dofs on the interface\n",
    "    \n",
    "    E_1 (sps.csc_matrix ) - sparse matrix that rearranges the dofs on domain 1\n",
    "    E_2 (sps.csc_matrix) - sparse matrix that rearranges the dofs on domain 2\n",
    "    E_interface (sps.csc_matrix) - sparse matrix that rearranges the dofs on the interface\n",
    "    \n",
    "    mesh (dolfin.cpp.generation.UnitSquareMesh) - contains the mesh \n",
    "    gridsize (positive integer) - number of dofs per row on the mesh\n",
    "    \n",
    "    Outputs:\n",
    "    \n",
    "    u_reshape (numpy array) - array ready to plot\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    U_nodal = block_vector_to_nodal(U,rows_domain1,rows_domain2,rows_interface,E_1,E_2,E_interface)\n",
    "    U_vertex = map_nodal_to_vertex(U_nodal,mesh)\n",
    "    u_reshape = np.reshape(U_vertex,(gridsize+1, gridsize+1))\n",
    "    \n",
    "    u_reshape = np.flipud(u_reshape)\n",
    "    \n",
    "    return u_reshape\n",
    "    \n",
    "def nodal_to_figure(u, gridsize):\n",
    "    \n",
    "    \"\"\"\n",
    "    Creates a figure from the nodal values\n",
    "    \n",
    "    Inputs: \n",
    "    \n",
    "    u_numpy (numpy array) - nodal values\n",
    "    gridsize (integer) - size of the grid\n",
    "    \n",
    "    Outputs:\n",
    "    \n",
    "    Figure of the solution\n",
    "    \n",
    "    \"\"\"\n",
    "    # map from nodal to vertex values\n",
    "    u_vertex = map_nodal_to_vertex(u, mesh)\n",
    "\n",
    "    u_reshape = np.reshape(u_vertex,(gridsize+1, gridsize+1))\n",
    "    u_reshape = np.flipud(u_reshape)\n",
    "\n",
    "    # Plot\n",
    "    plt.imshow(u_reshape, interpolation = \"nearest\",extent=[0,1,0,1])\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    \n",
    "def split_data(data, train, val, test):\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    Splits the data set in train val en test set respectively\n",
    "    \n",
    "    Inputs: \n",
    "    \n",
    "    data (numpy array) - data\n",
    "    train (value between 0 and 1) - ratio of training data\n",
    "    val (value between 0 and 1) - ratio of validation data\n",
    "    test (value between 0 and 1) - ratio of test data\n",
    "    \n",
    "    Outputs: \n",
    "    \n",
    "    train_data (numpy array) - training data\n",
    "    val_data (numpy array) - validation data\n",
    "    test_data (numpy array) - test data\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    train_len = int(np.floor(train*len(data)))\n",
    "    val_len = int(np.floor(val*len(data)))\n",
    "    test_len = int(np.floor(test*len(data)))\n",
    "\n",
    "    train_data = data[:train_len]\n",
    "    val_data = data[train_len:train_len+val_len]\n",
    "    test_data = data[-test_len:]\n",
    "\n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "def plot_last_epochs(n, train_losses_epochs, val_losses_epochs):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Plots the training and validation loss of the last n epochs\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    train_losses_epochs (numpy array) - array of training loss per epoch\n",
    "    val_losses_epoch (numpy array) - array of validation loss per epoch\n",
    "    n (positive integer) - number of last epochs to be shown\n",
    "    \n",
    "    Outputs:\n",
    "    \n",
    "    Figure of the last n epochs\n",
    "    \n",
    "    \n",
    "    \"\"\" \n",
    "    xaxis = list(range(len(train_losses_epochs)-n, len(train_losses_epochs)))\n",
    "    plt.plot(xaxis,train_losses_epochs[-n:], label='training loss')\n",
    "    plt.plot(xaxis,val_losses_epochs[-n:], label='validation loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend()\n",
    "    plt.xlabel('epoch')\n",
    "    plt.show()\n",
    "    \n",
    "def relative_error(base, approximation):\n",
    "    \n",
    "    \"\"\"\n",
    "    Calculates the 2-norm relative error between base and approximation\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    base (numpy array) - array to compare with approximation\n",
    "    approximation (numpy array) - approximation to base\n",
    "    \n",
    "    Outputs:\n",
    "    \n",
    "    rel_error (float) - relative error between base and approximation\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    rel_error = my_2norm(approximation-base)/my_2norm(base)\n",
    "    \n",
    "    return rel_error\n",
    "    \n",
    "\n",
    "def my_2norm(v):\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Calculates the Euclidean norm of a vector\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    v (numpy array) - vector\n",
    "    \n",
    "    Outputs:\n",
    "    \n",
    "    (float) - Euclidean norm of v\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    mynorm = 0\n",
    "    for i in range(len(v)):\n",
    "        val = (v[i])**2\n",
    "        mynorm = mynorm + val\n",
    "        \n",
    "    return sqrt(mynorm)\n",
    "\n",
    "\n",
    "def get_inputs(mu_inputs, I_inputs, normalized):\n",
    "    \"\"\"\n",
    "    Gives the inputs for the POD-NN on domain 2\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    mu_inputs (numpy array) - array of parameters\n",
    "    I_inputs (numpy array) - array of interface values\n",
    "    normalized (boolean) - normalized inputs yes or no\n",
    "    \n",
    "    Outputs:\n",
    "    \n",
    "    torch_inputs (torch) - inputs for the neural network\n",
    "    if normalized:\n",
    "        maxvalues_in (numpy array) - stores the max values per array\n",
    "        minvalues_in (numpy array) - stores the min values per array\n",
    "    \n",
    "    \"\"\"\n",
    "    inputs = np.hstack([I_inputs, mu_inputs])\n",
    "    torch_inputs = torch.from_numpy(inputs).float()\n",
    "    \n",
    "    if normalized:\n",
    "        normalized_inputs = np.zeros((np.shape(inputs)[0],np.shape(inputs)[1]))\n",
    "        maxvalues_in = np.zeros(np.shape(inputs)[1])\n",
    "        minvalues_in = np.zeros(np.shape(inputs)[1])\n",
    "\n",
    "        for i in range(np.shape(inputs)[1]):\n",
    "            nvector, maxval, minval = normalize(inputs[:,i])\n",
    "            normalized_inputs[:,i] = nvector\n",
    "            maxvalues_in[i] = maxval\n",
    "            minvalues_in[i] = minval\n",
    "            \n",
    "        torch_inputs = torch.from_numpy(normalized_inputs).float()\n",
    "        \n",
    "        return torch_inputs, maxvalues_in, minvalues_in\n",
    "        \n",
    "    else:\n",
    "        return torch_inputs\n",
    "            \n",
    "    \n",
    "def get_outputs(s_2,V_2, normalized):\n",
    "    \"\"\"\n",
    "    Gives the outputs for the POD-NN on domain 2\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    s_2 (numpy array) - snapshot matrix of hybrid model on domain 2\n",
    "    V_2 (numpy array) - transformation matrix of domain 2\n",
    "    normalized (boolean) - normalized inputs yes or no\n",
    "    \n",
    "    Outputs:\n",
    "      \n",
    "    torch_outputs (torch) - torch of the outputs of the neural network\n",
    "    if normalized:\n",
    "        maxvalues_in (numpy array) - stores the max values per array\n",
    "        minvalues_in (numpy array) - stores the min values per array\n",
    "    \n",
    "    \"\"\"\n",
    "    outputs = np.matmul(s_2.T,V_2)\n",
    "    torch_outputs = torch.from_numpy(outputs).float()\n",
    "    \n",
    "    if normalized:\n",
    "        normalized_outputs = np.zeros((np.shape(outputs)[0],np.shape(outputs)[1]))\n",
    "        maxvalues_out = np.zeros(np.shape(outputs)[1])\n",
    "        minvalues_out = np.zeros(np.shape(outputs)[1])\n",
    "\n",
    "        for i in range(np.shape(outputs)[1]):\n",
    "            nvector, maxval, minval = normalize(outputs[:,i])\n",
    "            normalized_outputs[:,i] = nvector    \n",
    "            maxvalues_out[i] = maxval\n",
    "            minvalues_out[i] = minval\n",
    "            \n",
    "        torch_outputs = torch.from_numpy(normalized_outputs).float()\n",
    "        \n",
    "        return torch_outputs, maxvalues_out, minvalues_out\n",
    "    \n",
    "    else:\n",
    "        return torch_outputs\n",
    "    \n",
    "\n",
    "\n",
    "def input_to_output_model_nonorm(u_I,mu,model):\n",
    "    \"\"\"\n",
    "    For the neural networks, giving the inputs we get the output\n",
    "    \n",
    "    Inputs:\n",
    "    u_I (numpy array) - array of interface values\n",
    "    mu (numpy array) - array of parameter values\n",
    "    model (torch.model) - corresponding neural network\n",
    "    \n",
    "    Outputs:\n",
    "    output (numpy array) - prediction\n",
    "    \"\"\"\n",
    "    \n",
    "    inputs = np.hstack((u_I,mu))\n",
    "    output = model(torch.from_numpy(inputs).float()).detach().numpy()  \n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "def find_initial_condition(mu, mu_test, u_I_list, nsamples, ntest):\n",
    "    \"\"\"\n",
    "    Finds the initial condition u_I depending on the closest mu\n",
    "    \n",
    "    Inputs:\n",
    "    mu (numpy array) - array of parameter locations\n",
    "    mu_test (numpy array) - parameter location of interest\n",
    "    u_I_list (numpy array) - array of reduced interface vectors\n",
    "    nsamples (int) - number of samples\n",
    "    ntest (int) - number of test samples\n",
    "    \n",
    "    Outputs:\n",
    "    closest_u_I (numpy array) - initial condition\n",
    "    mu_norm (float) - 2-norm between closest mu and mu_test\n",
    "    mu[index] - parameter location corresponding to initial condition\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    closest = np.linalg.norm(mu_test)\n",
    "    for i in range(nsamples-ntest):\n",
    "        candidate = np.linalg.norm(mu_test-mu[i])\n",
    "        if candidate < closest:\n",
    "            closest = candidate\n",
    "            closest_u_I = u_I_list[i]\n",
    "            index = i\n",
    "            \n",
    "    mu_norm = np.linalg.norm(mu[index]-mu_test)\n",
    "            \n",
    "    return closest_u_I, mu_norm, mu[index]\n",
    "\n",
    "\n",
    "\n",
    "def newdescent(x_k, mu, h, gamma, a_tilde, f_tilde, threshold, traction_model):\n",
    "    \"\"\"\n",
    "    Solves the non-linear equation in the online stage with unnormalized neural networks using gradient descent\n",
    "        \n",
    "    Inputs:\n",
    "    x_k (numpy array) - initial condition\n",
    "    mu (numpy array) - array of parameter values\n",
    "    h (float) - used for finite differences\n",
    "    a_tilde (numpy array) - input matrix in non-linear equation\n",
    "    f_tilde (numpy array) - input vector in non-linear equation\n",
    "    threshold (float) - threshold for the stopping criterion\n",
    "    \n",
    "    Outputs:\n",
    "    x_k (numpy array) - solution vector\n",
    "    delta_norm (float) - l2-norm between the last two iterations\n",
    "    F_k (numpy array) - vector F in the last iteration\n",
    "    count (integer) - number of iterations\n",
    "    residuals (numpy array) - array of residuals \n",
    "    \"\"\"\n",
    "        \n",
    "    count = 0\n",
    "    \n",
    "    residuals = []\n",
    "        \n",
    "    delta_norm = 1+threshold\n",
    "    \n",
    "    while delta_norm > threshold:\n",
    "        \n",
    "            \n",
    "        t_hat_k = input_to_output_model_nonorm(x_k,mu,traction_model)\n",
    "\n",
    "        # F_k for F_k(x_k)        \n",
    "        F_k = np.matmul(a_tilde,x_k) + f_tilde - t_hat_k\n",
    "        residuals.append(np.linalg.norm(F_k))\n",
    "\n",
    "        # initialize Jacobian in step k by J_k\n",
    "        J_k = np.ones((len(F_k),len(F_k)))\n",
    "\n",
    "        for i in range(len(F_k)):\n",
    "\n",
    "            # Determine x+h input\n",
    "            x_kh = copy.deepcopy(x_k)\n",
    "\n",
    "            x_kh[i] = x_k[i]+h\n",
    "\n",
    "            t_hat_kh = input_to_output_model_nonorm(x_kh,mu,traction_model)\n",
    "\n",
    "            value = (t_hat_kh-t_hat_k)/h\n",
    "            J_k[:,i] = value\n",
    "        \n",
    "        J_k = a_tilde - J_k\n",
    "        \n",
    "        # Start of Backtracking Line Search to find a suitable step size\n",
    "        \n",
    "        # Define search control parameters\n",
    "        \n",
    "        c = 0.5\n",
    "        tau = 0.5\n",
    "        a = 10\n",
    "\n",
    "        # Compute the gradient\n",
    "        gradG = np.matmul(J_k.T,F_k)\n",
    "        \n",
    "        # Compute some parameters\n",
    "        m = -np.linalg.norm(np.matmul(gradG,gradG))\n",
    "        t = -c*m\n",
    "        \n",
    "        x_temp = x_k-a*gradG\n",
    "        t_hat_temp = input_to_output_model_nonorm(x_temp,mu,traction_model)\n",
    "        F_temp = np.matmul(a_tilde,x_temp) + f_tilde - t_hat_temp\n",
    "        \n",
    "        while np.linalg.norm(F_k)-np.linalg.norm(F_temp) < a*t:\n",
    "            \n",
    "            # update\n",
    "            a = tau*a\n",
    "            x_temp = x_k-a*gradG\n",
    "            \n",
    "            t_hat_temp = input_to_output_model_nonorm(x_temp,mu,traction_model)\n",
    "            F_temp = np.matmul(a_tilde,x_temp) + f_tilde - t_hat_temp\n",
    "            \n",
    "        # Return step size gamma\n",
    "        gamma = a\n",
    "        \n",
    "        # End of Backtracking line search\n",
    "        \n",
    "        # Determine x_k1\n",
    "        x_k1 = x_k - gamma*gradG\n",
    "        \n",
    "        delta_norm = np.linalg.norm(x_k-x_k1)\n",
    "        \n",
    "        x_k = x_k1\n",
    "        count = count + 1\n",
    "        \n",
    "        \n",
    "        if delta_norm > 10e6:\n",
    "            return 'No convergence', 'No convergence', 'No convergence', 'No convergence', 'No convergence'\n",
    "\n",
    "    return x_k, delta_norm, F_k, count, residuals\n",
    "\n",
    "\n",
    "def newton_nonorm(x_k, mu, h, a_tilde, f_tilde, criterion, threshold, traction_model):\n",
    "    \"\"\"\n",
    "    Solves the non-linear equation in the online stage with unnormalized neural networks using newton's method\n",
    "        \n",
    "    Inputs:\n",
    "    x_k (numpy array) - initial condition\n",
    "    mu (numpy array) - array of parameter values\n",
    "    h (float) - used for finite differences\n",
    "    a_tilde (numpy array) - input matrix in non-linear equation\n",
    "    f_tilde (numpy array) - input vector in non-linear equation\n",
    "    criterion (boolean) - 1 is means Newton's algorithm stops when difference between the last two computed solutions\n",
    "                          is smaller than the threshold\n",
    "                          0 means Newton's algorithm stops when F(x_k) is smaller than the threshold\n",
    "    threshold (float) - threshold for the criterion\n",
    "    \n",
    "    Outputs:\n",
    "    x_k (numpy array) - solution vector\n",
    "    delta_norm (float) - l2-norm between the last two iterations\n",
    "    F_k (numpy array) - vector F in the last iteration\n",
    "    count (integer) - number of iterations\n",
    "    \"\"\"\n",
    "    \n",
    "    count = 0\n",
    "    residuals = []\n",
    "    \n",
    "    t_hat_k = input_to_output_model_nonorm(x_k,mu,traction_model)\n",
    "\n",
    "    # F_k for F_k(x_k)\n",
    "    F_k = np.matmul(a_tilde,x_k) + f_tilde - t_hat_k\n",
    "    \n",
    "    if criterion:\n",
    "        delta_norm = np.linalg.norm(delta)\n",
    "    else: \n",
    "        delta_norm = np.linalg.norm(F_k)\n",
    "        \n",
    "    while delta_norm > threshold:\n",
    "\n",
    "        t_hat_k = input_to_output_model_nonorm(x_k,mu,traction_model)\n",
    "\n",
    "        # F_k for F_k(x_k)\n",
    "        F_k = np.matmul(a_tilde,x_k) + f_tilde - t_hat_k\n",
    "\n",
    "        # initialize Jacobian in step k by J_k\n",
    "        J_k = np.ones((len(F_k),len(F_k)))\n",
    "\n",
    "        for i in range(len(F_k)):\n",
    "\n",
    "            # Determine x+h input\n",
    "            x_kh = copy.deepcopy(x_k)\n",
    "\n",
    "            x_kh[i] = x_k[i]+h\n",
    "\n",
    "            t_hat_kh = input_to_output_model_nonorm(x_kh,mu,traction_model)\n",
    "\n",
    "            # Get F_kh\n",
    "            F_kh = np.matmul(a_tilde,x_kh) + f_tilde - t_hat_kh\n",
    "\n",
    "            value = (F_kh-F_k)/h\n",
    "            J_k[:,i] = value\n",
    "\n",
    "\n",
    "        # Determine x_k1-xk\n",
    "        delta = np.linalg.solve(J_k,-F_k)\n",
    "        \n",
    "        if criterion:\n",
    "            delta_norm = np.linalg.norm(delta)\n",
    "        else: \n",
    "            delta_norm = np.linalg.norm(F_k)\n",
    "            residuals.append(delta_norm)\n",
    "\n",
    "            \n",
    "        x_k1 = delta+x_k\n",
    "\n",
    "        # update\n",
    "        x_k = x_k1\n",
    "        count = count + 1\n",
    "\n",
    "        if delta_norm > 10e6:\n",
    "            print('No convergence')\n",
    "            \n",
    "            if count > 1000:\n",
    "                return x_k, delta_norm, F_k, 1000, residuals[0:1000]\n",
    "            else: \n",
    "                return x_k, delta_norm, F_k, count, residuals\n",
    "\n",
    "    return x_k, delta_norm, F_k, count, residuals\n",
    "\n",
    "def Neural_Network(torch_inputs, torch_outputs, n_epochs, learning_rate):\n",
    "    \n",
    "    \"\"\"\n",
    "    Constructs and trains a neural network\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    torch_inputs (torch) - inputs for the neural network\n",
    "    torch_outputs (torch) - outputs fo the neural network\n",
    "    n_epochs (integer) - number of epochs\n",
    "    learning_rate (float) - learning rate\n",
    "    \n",
    "    Outputs:\n",
    "    \n",
    "    model (sequential) - neural network architecture\n",
    "    NNtime (float) - running time\n",
    "    test_loss (float) - test loss\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # Split data\n",
    "\n",
    "    train = 0.8\n",
    "    val = 0.1\n",
    "    test = 0.1\n",
    "\n",
    "    train_data_x, val_data_x, test_data_x = split_data(torch_inputs,train,val,test)\n",
    "    train_data_y, val_data_y, test_data_y = split_data(torch_outputs,train,val,test)\n",
    "\n",
    "    n_input = np.shape(train_data_x)[1]\n",
    "    n_hidden = 50\n",
    "    n_out = np.shape(train_data_y)[1]\n",
    "\n",
    "    # Define structure of the NN\n",
    "    model = nn.Sequential(nn.Linear(n_input, n_hidden),\n",
    "                          nn.LeakyReLU(),\n",
    "                          nn.Linear(n_hidden,n_hidden),\n",
    "                          nn.LeakyReLU(),\n",
    "                          nn.Linear(n_hidden,n_hidden),\n",
    "                          nn.LeakyReLU(),\n",
    "                          nn.Linear(n_hidden,n_hidden),\n",
    "                          nn.LeakyReLU(),\n",
    "                          nn.Linear(n_hidden, n_out))\n",
    "\n",
    "    batch_size = 16\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    loss_function = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    x = train_data_x\n",
    "    y = train_data_y\n",
    "\n",
    "    model = train_nn(model, train_data_x, train_data_y, val_data_x, val_data_y, n_epochs, batch_size, loss_function, optimizer)\n",
    "\n",
    "    end = time.time()\n",
    "    NNtime = end-start\n",
    "\n",
    "    # Determine test loss\n",
    "    model.eval()\n",
    "    y_pred = model(test_data_x)\n",
    "\n",
    "    test_loss = loss_function(y_pred, test_data_y) \n",
    "\n",
    "    y_predn = model(test_data_x).detach().numpy()\n",
    "    test_data_yn = test_data_y.detach().numpy()\n",
    "\n",
    "    avgrel = 0\n",
    "    \n",
    "    for i in range(len(y_pred)):\n",
    "        error = relative_error(y_predn[i],test_data_yn[i])\n",
    "        avgrel = avgrel + error\n",
    "        \n",
    "    \n",
    "    return model, NNtime, test_loss\n",
    "\n",
    "def Neural_Network_small(torch_inputs, torch_outputs, n_epochs, learning_rate):\n",
    "    \n",
    "    \"\"\"\n",
    "    Constructs and trains a small neural network\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    torch_inputs (torch) - inputs for the neural network\n",
    "    torch_outputs (torch) - outputs fo the neural network\n",
    "    n_epochs (integer) - number of epochs\n",
    "    learning_rate (float) - learning rate\n",
    "    \n",
    "    Outputs:\n",
    "    \n",
    "    model (sequential) - neural network architecture\n",
    "    NNtime (float) - running time\n",
    "    test_loss (float) - test loss\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # Split data\n",
    "\n",
    "    train = 0.8\n",
    "    val = 0.1\n",
    "    test = 0.1\n",
    "\n",
    "    train_data_x, val_data_x, test_data_x = split_data(torch_inputs,train,val,test)\n",
    "    train_data_y, val_data_y, test_data_y = split_data(torch_outputs,train,val,test)\n",
    "\n",
    "    n_input = np.shape(train_data_x)[1]\n",
    "    n_hidden = 20\n",
    "    n_out = np.shape(train_data_y)[1]\n",
    "\n",
    "    # Define structure of the NN\n",
    "    model = nn.Sequential(nn.Linear(n_input, n_hidden),\n",
    "                          nn.LeakyReLU(),\n",
    "                          nn.Linear(n_hidden,n_hidden),\n",
    "                          nn.LeakyReLU(),\n",
    "                          nn.Linear(n_hidden, n_out))\n",
    "\n",
    "    batch_size = 16\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    loss_function = nn.MSELoss()\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    x = train_data_x\n",
    "    y = train_data_y\n",
    "\n",
    "    model = train_nn(model, train_data_x, train_data_y, val_data_x, val_data_y, n_epochs, batch_size, loss_function, optimizer)\n",
    "\n",
    "    end = time.time()\n",
    "    NNtime = end-start\n",
    "\n",
    "    # Determine test loss\n",
    "    model.eval()\n",
    "    y_pred = model(test_data_x)\n",
    "\n",
    "    test_loss = loss_function(y_pred, test_data_y) \n",
    "\n",
    "    y_predn = model(test_data_x).detach().numpy()\n",
    "    test_data_yn = test_data_y.detach().numpy()\n",
    "\n",
    "    avgrel = 0\n",
    "    \n",
    "    for i in range(len(y_pred)):\n",
    "        error = relative_error(y_predn[i],test_data_yn[i])\n",
    "        avgrel = avgrel + error\n",
    "            \n",
    "    return model, NNtime, test_loss.detach().numpy()\n",
    "\n",
    "def train_nn(model, x, y, val_data_x, val_data_y, n_epochs, batch_size, lossf, opt):\n",
    "    \n",
    "    \"\"\"\n",
    "    Trains a neural network with validation data and early stopping algorithm\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    model (sequential) - neural network architecture\n",
    "    x (torch) - input training data\n",
    "    y (torch) - output training data\n",
    "    val_data_x (torch) - input validation data\n",
    "    val_data_y (torch) - output validation data\n",
    "    n_epochs (integer) - number of epochs\n",
    "    batch_size (integer) - batch size\n",
    "    lossf (function) - loss function\n",
    "    opt (function) - optimization function\n",
    "  \n",
    "    Outputs:\n",
    "    \n",
    "    best model (function) - returns neural network\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # For early stopping\n",
    "    val_loss_0 = 1e10\n",
    "    \n",
    "    n_batches = int(len(x) / batch_size)\n",
    "\n",
    "    # Initialize loss lists\n",
    "    train_losses_steps = []\n",
    "    train_losses_epochs = []\n",
    "    \n",
    "    val_losses_epochs = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        # Initialize epoch loss\n",
    "        epochloss = 0\n",
    "        \n",
    "        print(\"Current epoch: \"+ str(epoch+1), end=\"\\r\")\n",
    "\n",
    "        # Training loop\n",
    "        for batch in range(n_batches):\n",
    "            \n",
    "            # Create batches\n",
    "            batch_X, batch_y = x[batch*batch_size:(batch+1)*batch_size,], y[batch*batch_size:(batch+1)*batch_size,]\n",
    "\n",
    "            # Make prediction on the batch and save the loss\n",
    "            pred_y = model(batch_X)\n",
    "            loss = lossf(pred_y, batch_y)\n",
    "\n",
    "            # Add loss to the array and sum the loss to calculate loss per epoch\n",
    "            train_losses_steps.append(loss.item())\n",
    "            epochloss += loss\n",
    "\n",
    "            # Backward propagation\n",
    "            model.zero_grad()\n",
    "            loss.backward() \n",
    "\n",
    "            # Update weights and biases\n",
    "            opt.step()\n",
    "\n",
    "      # Validation loop\n",
    "        with torch.no_grad(): \n",
    "            model.eval() \n",
    "\n",
    "            pred_y = model(val_data_x) \n",
    "            val_loss = lossf(pred_y, val_data_y) \n",
    "            val_losses_epochs.append(val_loss)\n",
    "            \n",
    "            # Early stopping\n",
    "            if val_loss < val_loss_0:\n",
    "                \n",
    "                best_model = model\n",
    "                best_epoch = epoch\n",
    "                val_loss_0 = val_loss\n",
    "\n",
    "        # Append average epoch loss\n",
    "        train_losses_epochs.append(epochloss.item()/n_batches)\n",
    "\n",
    "    print(\"\\n Best epoch:\",best_epoch)\n",
    "\n",
    "    # Plot loss\n",
    "#     plt.plot(train_losses_epochs, label='training loss')\n",
    "#     plt.plot(val_losses_epochs, label='validation loss')\n",
    "\n",
    "#     plt.ylabel('MSE loss')\n",
    "#     plt.legend()\n",
    "#     plt.xlabel('epoch')\n",
    "#     plt.show()\n",
    "    \n",
    "    return best_model\n",
    "\n",
    "def BFGS_Fu(x,mu_test,a1_tilde,f1_tilde, traction_model):\n",
    "    \n",
    "    \"\"\"\n",
    "    Computes the function value of F(u_\\Gamma) as in (3.18)\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    x (numpy array) - reduced-interface vector\n",
    "    mu_test (numpy array) - parameter location\n",
    "    a1_tilde (numpy array) - a^{(1)}_{ΓΓ} − a_{Γ1}a^{−1}_{11} a_{1Γ}\n",
    "    f1_tilde (numpy array) - a_{Γ1}a^{−1}_{11} f_1 − f^{(1)}_Γ\n",
    "    traction_model (mapping) - mapping of the traction model\n",
    "\n",
    "    Outputs:\n",
    "    \n",
    "    (numpy array) Function value at x\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    t_hat = input_to_output_model_nonorm(x,mu_test,traction_model)\n",
    "    \n",
    "    return np.matmul(a1_tilde,x) + f1_tilde - t_hat \n",
    "\n",
    "def BFGS_g(x, mu_test, a1_tilde, f1_tilde, traction_model):\n",
    "    \n",
    "    \"\"\"\n",
    "    Objective function as in (3.26) for BFGS algorithm\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    x (numpy array) - reduced interface vector\n",
    "    mu_test (numpy array) - parameter location\n",
    "    a1_tilde (numpy array) - a^{(1)}_{ΓΓ} − a_{Γ1}a^{−1}_{11} a_{1Γ}\n",
    "    f1_tilde (numpy array) - a_{Γ1}a^{−1}_{11} f_1 − f^{(1)}_Γ\n",
    "    traction_model (mapping) - mapping of the traction model\n",
    "    \n",
    "    Outputs:\n",
    "    \n",
    "    g(x) (numpy array) - objective\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    return 0.5*np.dot(BFGS_Fu(x,mu_test,a1_tilde,f1_tilde, traction_model).T,BFGS_Fu(x,mu_test,a1_tilde,f1_tilde,traction_model))\n",
    "\n",
    "def BFGS_grad_g(x, h, mu_test, a1_tilde, f1_tilde, traction_model):\n",
    "    \n",
    "    \"\"\"\n",
    "    Computes the gradient at x using first-order finite differences for the BFGS algorithm\n",
    "    \n",
    "    Inputs: \n",
    "    \n",
    "    x (numpy array) - location to compute the gradient\n",
    "    h (float) - parameter for finite differences\n",
    "    h (float) - parameter for finite differences\n",
    "    mu_test (numpy array) - parameter location\n",
    "    a1_tilde (numpy array) - a^{(1)}_{ΓΓ} − a_{Γ1}a^{−1}_{11} a_{1Γ}\n",
    "    f1_tilde (numpy array) - a_{Γ1}a^{−1}_{11} f_1 − f^{(1)}_Γ\n",
    "    traction_model (mapping) - mapping of the traction model\n",
    "    \n",
    "    Outputs:\n",
    "    \n",
    "    grad (numpy array) - gradient at x\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # Initialize\n",
    "    grad = np.zeros((len(x)))\n",
    "    \n",
    "    # Loop through arguments of g\n",
    "    for i in range(len(x)):\n",
    "        \n",
    "        x_h = copy.deepcopy(x)  \n",
    "        \n",
    "        # Calculate the derivative using finite differences\n",
    "        x_h[i] = x[i]+h\n",
    "        val = (BFGS_g(x_h, mu_test, a1_tilde, f1_tilde, traction_model)-BFGS_g(x, mu_test, a1_tilde, f1_tilde, traction_model))/h\n",
    "        \n",
    "        # Store\n",
    "        grad[i] = val\n",
    "    \n",
    "    return grad\n",
    "\n",
    "def BFGS_line_search(c, tau, a, x, p, h, mu_test, a1_tilde, f1_tilde, traction_model):\n",
    "    \n",
    "    \"\"\"\n",
    "    Performs the backtracking line search algorithm for the BFGS algorithm\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    c, tau (float) - control parameters\n",
    "    a (float) - initial step size\n",
    "    x (numpy array) - location \n",
    "    p (numpy array) - descent direction\n",
    "    h (float) - parameter for finite differences\n",
    "    mu_test (numpy array) - parameter location\n",
    "    a1_tilde (numpy array) - a^{(1)}_{ΓΓ} − a_{Γ1}a^{−1}_{11} a_{1Γ}\n",
    "    f1_tilde (numpy array) - a_{Γ1}a^{−1}_{11} f_1 − f^{(1)}_Γ\n",
    "    traction_model (mapping) - mapping of the traction model\n",
    "    \n",
    "    Outputs:\n",
    "    \n",
    "    a (float) - optimized step size\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute some parameters\n",
    "    m = np.matmul(BFGS_grad_g(x, h, mu_test, a1_tilde, f1_tilde, traction_model),p)\n",
    "    t = -c*m\n",
    "\n",
    "    while BFGS_g(x, mu_test, a1_tilde, f1_tilde, traction_model) - BFGS_g(x+a*p, mu_test, a1_tilde, f1_tilde, traction_model) < a*t:\n",
    "\n",
    "        # update\n",
    "        a = tau*a\n",
    "\n",
    "    # Return step size\n",
    "    return a   \n",
    "\n",
    "def BFGS(x_k, H_k, threshold, h, mu_test, a1_tilde, f1_tilde, traction_model):\n",
    "    \"\"\"\n",
    "    Performs the BFGS algorithm with stopping criterion based on the norm of the gradient of G\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    x_k (numpy array) - reduced interface vector\n",
    "    H_k (numpy array) - Initial guess Hessian\n",
    "    threshold (float) - threshold for norm of grad(G)\n",
    "    h (float) - parameter for finite differences\n",
    "    mu_test (numpy array) - parameter location\n",
    "    a1_tilde (numpy array) - a^{(1)}_{ΓΓ} − a_{Γ1}a^{−1}_{11} a_{1Γ}\n",
    "    f1_tilde (numpy array) - a_{Γ1}a^{−1}_{11} f_1 − f^{(1)}_Γ\n",
    "    traction_model (mapping) - mapping of the traction model\n",
    "    \n",
    "    Outputs: \n",
    "    \n",
    "    x_k (numpy array) - reduced interface vector\n",
    "    residuals (numpy array) - residuals per iteration\n",
    "    count (integer) - number of iterations\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    count = 0\n",
    "    delta_norm = 1+threshold\n",
    "    residuals = []\n",
    "\n",
    "    while delta_norm > threshold:\n",
    "\n",
    "        residual = np.linalg.norm(BFGS_Fu(x_k,mu_test,a1_tilde,f1_tilde, traction_model))\n",
    "\n",
    "        # Determine Gradient at x_k\n",
    "        grad_G = BFGS_grad_g(x_k, h, mu_test, a1_tilde, f1_tilde, traction_model)\n",
    "\n",
    "        # Determine search direction\n",
    "        p_k = np.matmul(-H_k, grad_G)\n",
    "\n",
    "        # Perform back-tracking line search\n",
    "        lr = BFGS_line_search(0.5, 0.5, 10, x_k, p_k, h, mu_test, a1_tilde, f1_tilde, traction_model)\n",
    "\n",
    "        s_k = lr*p_k\n",
    "        x_kp1 = x_k+s_k\n",
    "\n",
    "\n",
    "        # Determine Gradient at x_kp1\n",
    "        grad_G_h = BFGS_grad_g(x_kp1,h, mu_test, a1_tilde, f1_tilde, traction_model)\n",
    "\n",
    "        y_k = grad_G_h-grad_G\n",
    "\n",
    "        # Approximate the Hessian\n",
    "        rho_k = 1/np.dot(y_k.T,s_k)\n",
    "        H_kp1 = np.dot(np.dot(np.identity(len(x_k))-np.dot(np.dot(s_k,rho_k),y_k.T),H_k),np.identity(len(x_k))-np.dot(np.dot(rho_k,y_k),s_k.T))+np.dot(np.dot(rho_k,s_k),s_k.T)\n",
    "\n",
    "        # Update\n",
    "        H_k = H_kp1 \n",
    "        delta_norm = np.linalg.norm(y_k) \n",
    "\n",
    "        x_k = x_kp1\n",
    "        count = count + 1\n",
    "\n",
    "        # Store\n",
    "        residuals.append(residual)\n",
    "    \n",
    "    return x_k, residuals, count\n",
    "\n",
    "def Fu(x,mu_test,a1_tilde,f1_tilde):\n",
    "    \n",
    "    \"\"\"\n",
    "    Computes the function value of F(u_\\Gamma) as in (3.18)\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    x (numpy array) - reduced-interface vector\n",
    "    mu_test (numpy array) - parameter location\n",
    "    a1_tilde (numpy array) - a^{(1)}_{ΓΓ} − a_{Γ1}a^{−1}_{11} a_{1Γ}\n",
    "    f1_tilde (numpy array) - a_{Γ1}a^{−1}_{11} f_1 − f^{(1)}_Γ\n",
    "    \n",
    "    Outputs:\n",
    "    \n",
    "    (numpy array) Function value at x\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    t_hat = input_to_output_model_nonorm(x,mu_test,traction_model)\n",
    "    \n",
    "    return np.matmul(a1_tilde,x) + f1_tilde - t_hat \n",
    "\n",
    "def g(x):\n",
    "    \n",
    "    \"\"\"\n",
    "    Objective function as in (3.26)\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    x (numpy array) - reduced interface vector\n",
    "    \n",
    "    Outputs:\n",
    "    \n",
    "    g(x) (numpy array) - objective\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    return 0.5*np.dot(Fu(x,mu_test,a1_tilde,f1_tilde).T,Fu(x,mu_test,a1_tilde,f1_tilde))\n",
    "\n",
    "def grad_g(x, h):\n",
    "    \n",
    "    \"\"\"\n",
    "    Computes the gradient at x using first-order finite differences \n",
    "    \n",
    "    Inputs: \n",
    "    \n",
    "    x (numpy array) - location to compute the gradient\n",
    "    h (float) - parameter for finite differences\n",
    "    \n",
    "    Outputs:\n",
    "    \n",
    "    grad (numpy array) - gradient at x\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize]\n",
    "    grad = np.zeros((len(x)))\n",
    "    \n",
    "    # Loop through arguments of g\n",
    "    for i in range(len(x)):\n",
    "        \n",
    "        x_h = copy.deepcopy(x)  \n",
    "        \n",
    "        # Calculate the derivative using finite differences\n",
    "        x_h[i] = x[i]+h\n",
    "        val = (g(x_h)-g(x))/h\n",
    "        \n",
    "        # Store\n",
    "        grad[i] = val\n",
    "    \n",
    "    return grad\n",
    "\n",
    "def line_search(c, tau, a, x, p):\n",
    "    \n",
    "    \"\"\"\n",
    "    Performs the backtracking line search algorithm\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    c, tau (float) - control parameters\n",
    "    a (float) - initial step size\n",
    "    x (numpy array) - location \n",
    "    p (numpy array) - descent direction\n",
    "    \n",
    "    Outputs:\n",
    "    \n",
    "    a (float) - optimized step size\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute some parameters\n",
    "    m = np.matmul(grad_g(x,h),p)\n",
    "    t = -c*m\n",
    "\n",
    "    while g(x) - g(x+a*p) < a*t:\n",
    "\n",
    "        # update\n",
    "        a = tau*a\n",
    "\n",
    "    # Return step size\n",
    "    return a  \n",
    "\n",
    "\n",
    "def standardize(data):\n",
    "    \n",
    "    \"\"\"\n",
    "    Standardizes data\n",
    "    \n",
    "    Inputs: \n",
    "    \n",
    "    data (numpy array) - data to standardize\n",
    "    \n",
    "    Outputs:\n",
    "    \n",
    "    standardized_data (numpy array) - standardized data\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    standardized_data = np.zeros((np.shape(data)))\n",
    "    \n",
    "    means = np.mean(data,axis=0)\n",
    "    stds = np.std(data,axis=0)\n",
    "    \n",
    "    for idx, row in enumerate(data):\n",
    "        standardized_row = (row-means)/stds\n",
    "        standardized_data[idx] = standardized_row\n",
    "        \n",
    "    return standardized_data\n",
    "\n",
    "def normalize(data):\n",
    "    \n",
    "    \"\"\"\n",
    "    Normalizes data\n",
    "    \n",
    "    Inputs: \n",
    "    \n",
    "    data (numpy array) - data to normalize\n",
    "    \n",
    "    Outputs:\n",
    "    \n",
    "    normalized_data (numpy array) - normalized data\n",
    "    maxs (numpy array) - array of max values (to denormalize later)\n",
    "    mins (numpy array) - array of min values (to denormalize later)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    normalized_data = np.zeros((np.shape(data)))\n",
    "    \n",
    "    maxs = np.max(data,axis=0)\n",
    "    mins = np.min(data,axis=0)\n",
    "    \n",
    "    for idx, row in enumerate(data):\n",
    "        normalized_row = (row - mins)/(maxs - mins)\n",
    "        normalized_data[idx] = normalized_row\n",
    "        \n",
    "    return normalized_data, maxs, mins\n",
    "\n",
    "def Monte_Carlo(OoI_array, nsamples, mean : bool):\n",
    "    \n",
    "    \"\"\"\n",
    "    Performs Monte Carlo simulation for mean or variance\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    OoI_array (numpy array) - array of outputs of interest\n",
    "    nsamples (integer) - number of samples\n",
    "    mean (boolean) - 1 for mean, 0 for variance\n",
    "    \n",
    "    Outputs\n",
    "    \n",
    "    QoI_iter (numpy array) - quantity of interest per iteration\n",
    "    QoI_confidence_95_low_iter (numpy array) - lower bound confidence interval of QoI per iteration\n",
    "    QoI_confidence_95_high_iter (numpy array) - upper bound confidence interval of QoI per iteration\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    QoI_iter = np.zeros((nsamples-ntest))\n",
    "    QoI_confidence_95_low_iter = np.zeros((nsamples))\n",
    "    QoI_confidence_95_high_iter = np.zeros((nsamples))\n",
    "    \n",
    "    variance = np.var(OoI_array)\n",
    "    \n",
    "    for i in range(nsamples):\n",
    "            if mean:\n",
    "                QoI_iter[i] = np.mean(OoI_array[0:i+1])\n",
    "            else: \n",
    "                QoI_iter[i] = np.var(OoI_array[0:i+1])\n",
    "\n",
    "            QoI_confidence_95_low = QoI_iter[i]-1.96*sqrt(variance/(i+1)) \n",
    "            QoI_confidence_95_high = QoI_iter[i]+1.96*sqrt(variance/(i+1))\n",
    "\n",
    "            QoI_confidence_95_low_iter[i] = QoI_confidence_95_low\n",
    "            QoI_confidence_95_high_iter[i] = QoI_confidence_95_high\n",
    "            \n",
    "    # Figure \n",
    "    \n",
    "    xaxis = np.arange(100,nsamples-ntest)\n",
    "    fig = plt.plot(xaxis,QoI_iter[100:])\n",
    "    plt.plot(xaxis,QoI_confidence_95_low_iter[100:])\n",
    "    \n",
    "    plt.plot(xaxis,QoI_confidence_95_high_iter[100:])\n",
    "    plt.xlabel('Number of samples')\n",
    "    legend = plt.legend(['Quantity of Interest','Lower bound 95%-CI','Upper bound 95%-CI'])\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return QoI_iter, QoI_confidence_95_low_iter, QoI_confidence_95_high_iter\n",
    "\n",
    "def f_normal(x,mu,sigma):\n",
    "    \"\"\"\n",
    "    Probability density function of the normal distribution\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    x (float) - field variable\n",
    "    mu (float) - mean\n",
    "    sigma (float) - standard deviation\n",
    "    \n",
    "    Outputs: \n",
    "    \n",
    "    PDF value at x\n",
    "    \n",
    "    \"\"\"\n",
    "    return (1/(sigma*np.sqrt(2*np.pi)))*np.exp(-0.5*((x-mu)/sigma)**2)\n",
    "\n",
    "def uni_dist(x, a , b):\n",
    "    \n",
    "    \"\"\"\n",
    "    Probability density function of the uniform distribution\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "    x (float) - field variable\n",
    "    a, b (float) - left and right interval values\n",
    "    \n",
    "    Outputs: \n",
    "    \n",
    "    prob_density (float) - PDF value at x\n",
    "    \n",
    "    \"\"\"\n",
    "    if x >= a and x <= b:\n",
    "        prob_density = 1/(b-a)\n",
    "    else:\n",
    "        prob_density = 0\n",
    "    return prob_density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d57486",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
